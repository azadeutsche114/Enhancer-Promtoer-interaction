import tensorflow as tf
from tensorflow.keras.layers import Conv1D, Bidirectional, GRU, Dense, Input, Attention, GlobalMaxPooling1D, Concatenate
from tensorflow.keras.models import Model

# Define the DNA sequence input for both enhancer and promoter
seq_length = 1000  # Adjust based on your sequence length
input_enhancer = Input(shape=(seq_length, 4), name="Enhancer_Input")
input_promoter = Input(shape=(seq_length, 4), name="Promoter_Input")

# Function to create a CNN + GRU model for sequence feature extraction
def cnn_gru_block(input_seq):
    x = Conv1D(filters=32, kernel_size=5, activation="relu", padding="same")(input_seq)
    x = Bidirectional(GRU(32, return_sequences=True))(x)
    x = Attention()([x, x])  # Attention layer to focus on important features
    x = GlobalMaxPooling1D()(x)  # Pooling to get a fixed-size output
    return x

# Apply CNN-GRU block to both enhancer and promoter sequences
enhancer_features = cnn_gru_block(input_enhancer)
promoter_features = cnn_gru_block(input_promoter)

# Concatenate features and add dense layers for interaction prediction
combined_features = Concatenate()([enhancer_features, promoter_features])
x = Dense(64, activation="relu")(combined_features)
output = Dense(1, activation="sigmoid")(x)  # Binary classification output

# Define and compile the model
model = Model(inputs=[input_enhancer, input_promoter], outputs=output)
model.compile(optimizer="adam", loss="binary_crossentropy", metrics=["accuracy"])

# Summary of the model
model.summary()


import numpy as np

# one-hot encode DNA sequences
def one_hot_encode_sequence(sequence, seq_length):
    mapping = {'A': [1, 0, 0, 0], 'T': [0, 1, 0, 0], 'C': [0, 0, 1, 0], 'G': [0, 0, 0, 1]}
    encoded_seq = np.array([mapping[base] for base in sequence.upper() if base in mapping])
    if len(encoded_seq) < seq_length:
        padding = np.zeros((seq_length - len(encoded_seq), 4))
        encoded_seq = np.vstack((encoded_seq, padding))
    return encoded_seq[:seq_length]

# Example usage
enhancer_sequences = ["ACTG..."]  # Replace with actual DNA sequences
promoter_sequences = ["TGCA..."]  # Replace with actual DNA sequences

# Encoding sequences
X_enhancer = np.array([one_hot_encode_sequence(seq, seq_length) for seq in enhancer_sequences])
X_promoter = np.array([one_hot_encode_sequence(seq, seq_length) for seq in promoter_sequences])

# Binary labels indicating interaction (1) or non-interaction (0)
y = np.array([1, 0, ...])  # Replace with actual labels

# Train the model
model.fit([X_enhancer, X_promoter], y, epochs=10, batch_size=32, validation_split=0.2)



import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report
from sklearn.cluster import KMeans


# Load data 

import pandas as pd

url = 'https://apollo.com/DATA/data.csv'
df = pd.read_csv(url)
print(df.head())

X = data.data
y = data.enhancer

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Initialize and train Random Forest
rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X_train, y_train)

# Predict and evaluate
y_pred_rf = rf.predict(X_test)
print("Random Forest Accuracy:", accuracy_score(y_test, y_pred_rf))


# Initialize and train KNN
knn = KNeighborsClassifier(n_neighbors=3)
knn.fit(X_train, y_train)

# Predict and evaluate
y_pred_knn = knn.predict(X_test)
print("KNN Accuracy:", accuracy_score(y_test, y_pred_knn))

# Initialize and train SVM
svm = SVC(kernel='linear')
svm.fit(X_train, y_train)

# Predict and evaluate
y_pred_svm = svm.predict(X_test)
print("SVM Accuracy:", accuracy_score(y_test, y_pred_svm))

# Initialize and train Logistic Regression
log_reg = LogisticRegression(max_iter=200)
log_reg.fit(X_train, y_train)

# Predict and evaluate
y_pred_log_reg = log_reg.predict(X_test)
print("Logistic Regression Accuracy:", accuracy_score(y_test, y_pred_log_reg))

# Initialize and train KMeans
kmeans = KMeans(n_clusters=3, random_state=42)
clusters = kmeans.fit_predict(X)

# Display clusters
print("Cluster assignments:", clusters)

print("Random Forest Classification Report:\n", classification_report(y_test, y_pred_rf))
print("KNN Classification Report:\n", classification_report(y_test, y_pred_knn))
print("SVM Classification Report:\n", classification_report(y_test, y_pred_svm))
print("Logistic Regression Classification Report:\n", classification_report(y_test, y_pred_log_reg))
